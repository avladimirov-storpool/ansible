# ansible hosts inventory file for StorPool deployment
# all variables can also be defined per node in the roles sections below
# NOTE: all boolean values must be declared as True/False (capital first letter only!)

[all:vars]
# global variables for all roles

# remote ssh variables for ansible
# if user!=root /etc/sudoers on nodes needs to be adjusted accordingly
#ansible_user='storpool'	# defaults use ssh_config
#ansible_user='root'		# defaults use ssh_config
#ansible_port=22		# defaults use ssh_config

# used by StorPool for deployments from bastion host
sp_cluster='ansible-test'
sp_custdir='/home/cust'
sp_toolsdir='/root/storpool'

[storpool:vars]
# storpool role specific variables

# where to get the StorPool release file from
# sp_release_file='/path/storpool-...'	# tgz release file stored on ansible deployment host
# sp_release_file='web'			# default - download release from StorPool
sp_release_file='web'

# StorPool version to be deployed (if sp_release_file='web')
sp_release='19.01'			# default

# deployment type
#sp_new_cluster=True		# new cluster deployment
#sp_new_cluster=False		# adding node(s) to existing cluster (default)
sp_new_cluster=True

# local path to storpool.conf
sp_config='/path/to/storpool.conf'

# perform node OS update (yum/apt update/upgrade)
#sp_update_system=True		# default

# automatically configure network interfaces based on storpool.conf vars
#sp_configure_network=False	# default
# overwrite existing ineterface configuration files (iface-genconf -o)
#sp_overwrite_iface_conf=False	# default

# set SELinux state (permissive or disabled)
#sp_selinux='disabled'		# default

# stop and disable NetworkManager
#sp_disable_nm=True		# default

# stop and disable firewalld/ufw (if False, will add ports for StorPool)
#sp_disable_fw=True		# default

# node is a Virtual Machine (skip tests)
#sp_vm=False			# default: try to deduct from CPU product_name

# generate a summary and pause after variable validation
#sp_summary_wait=True		# default

# reboot node after deployment (not implemented yet, maybe combine with kdump)
#sp_reboot=False		# default

# drive selection section
#
# devices matching _any_ of the conditions below will get initialized
# values can be obtained by running 'lsblk -do NAME,SIZE,MODEL' on the nodes
# NOTE: a matched device, already containing partitions, will trigger an error
# 	remove partitions manually or override with sp_drive_erase=True
#
# CAUTION: the drive_init procedure is not fully implemented yet
# - always creates only one partition per drive
# - no RAID controller detection, and private case scenarios
# - needs ability to specify storpool_initdisk parameters
#
# coma-separated list of devices to initialize
#sp_drives='nvme0n1,nvme1n1,sdb,sdc'
#
# coma-separated list of sizes of devices to initialize
#sp_drive_size='3.5T,1.8T,223.6G'
#
# coma-separated list of models of devices to initialize 
#sp_drive_model='ST2000NM003A,INTEL SSDPE2KX080T8,SAMSUNG MZ7LH3T8'
#
# init the drives even if they contain partitions (use with caution)
# StorPool partitions will never be touched regardless of this flag
#sp_drive_erase=False		# default

# sp_node_roles: comma-separated list of roles for the node(s) being deployed
# all roles will get '@block cli' services installed
# defining 'server' and 'client' will treat the node as hyperconverged
# available roles:
#
# client		# client hypervisor node
# server[:N]		# N - max instances (1 if omitted)
# iscsi
# mgmt
# bridge
#
#sp_node_roles='client'		# default
#sp_node_roles='server,iscsi'

# sp_cg_conf_extra: list of arguments to pass to storpool_cg conf
# see 'storpool_cg conf -h' for all available arguments
# examples:
#
#sp_cg_conf_extra='CORES=2 NUMA_OVERFLOW=1 SP_COMMON_LIMIT=2G SP_ALLOC_LIMIT=2G'
#sp_cg_conf_extra='SYSTEM_LIMIT=4G USER_LIMIT=2G MACHINE_LIMIT=4G'
#
# default if client+server in sp_node_roles
#sp_cg_conf_extra='CONVERGED=1'
#
# default otherwise
#sp_cg_conf_extra=''

[storpool]
# node hostnames and per-node variables
# if no variables are defined will use globals/defaults from above
# examples:
#
#sp-node.1  sp_node_roles='server:6,mgmt' sp_drive_size='2000398934016'
#sp-node.2  sp_node_roles='server:6,mgmt' sp_drive_size='2000398934016'
#sp-node.3  sp_node_roles='server:6,mgmt' sp_drive_size='2000398934016'
#sp-node.4  sp_node_roles='server:4' sp_drives='nvme0n1,nvme1n1,nvme2n1,nvme3n1'
#sp-node.5  sp_node_roles='server:4' sp_drives='nvme0n1,nvme1n1,nvme2n1,nvme3n1'
#sp-node.6  sp_node_roles='server:2' sp_drives='nvme0n1,nvme1n1'
#sp-node.11 sp_node_roles='server,iscsi,bridge' sp_drives='sdb,sdc'
#sp-node.12 sp_node_roles='server,iscsi,bridge' sp_drives='sdb,sdc'
#sp-node.13 sp_node_roles='server,iscsi,bridge' sp_drives='sdb,sdc'
#sp-node.21 sp_node_roles='client,onapp' sp_disable_fw=False
#sp-node.22 
#sp-node.23
#sp-node.31 sp_node_roles='bridge' sp_vm=True sp_configure_network=False
#sp-node.32 sp_vm=True sp_configure_network=False sp_disable_nm=False

# EOF
